{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5227958",
   "metadata": {},
   "source": [
    "# Rug Pull Detection — Notebook de Experimentación\n",
    "\n",
    "**Objetivo:** Explorar los datos, construir features y entrenar 3 modelos de clasificación para detectar rug pulls en pools Uniswap V2.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Carga de datos desde la Batch API (todos los 12 meses)\n",
    "2. Exploración con `ydata_profiling`\n",
    "3. Feature engineering + heurísticas de labeling\n",
    "4. Entrenamiento con Random Forest, XGBoost y LightGBM\n",
    "5. Registro de experimentos en MLflow → MinIO\n",
    "\n",
    "**Servicios:** Jupyter corre en Docker (`net_mlops`). La Batch API corre en el host.\n",
    "\n",
    "| Servicio | URL desde este notebook |\n",
    "|---|---|\n",
    "| Batch API | `http://host.docker.internal:8000` |\n",
    "| MLflow | `http://mlflow:5000` |\n",
    "| MinIO (S3) | `http://minio:9000` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a3170",
   "metadata": {},
   "source": [
    "---\n",
    "## Panel 0 — Imports y configuración global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b6e4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports OK\n"
     ]
    }
   ],
   "source": [
    "# ── Librerías estándar ────────────────────────────────────────────────────────\n",
    "import os\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "# ── Datos ─────────────────────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# ── Visualización ─────────────────────────────────────────────────────────────\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# ── ML ────────────────────────────────────────────────────────────────────────\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, classification_report,\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ── MLflow ────────────────────────────────────────────────────────────────────\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import mlflow.lightgbm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid', palette='muted')\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "\n",
    "print('✓ Imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddb3b13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuración OK\n",
      "  API     : http://host.docker.internal:8000\n",
      "  MLflow  : http://mlflow:5000\n",
      "  MinIO   : http://minio:9000\n"
     ]
    }
   ],
   "source": [
    "# ── Configuración global ──────────────────────────────────────────────────────\n",
    "\n",
    "# Servicios\n",
    "API_URL     = 'http://host.docker.internal:8000'\n",
    "MLFLOW_URI  = 'http://mlflow:5000'\n",
    "MINIO_URL   = 'http://minio:9000'\n",
    "\n",
    "# Credenciales MinIO\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = MINIO_URL\n",
    "os.environ['AWS_ACCESS_KEY_ID']      = 'admin'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']  = 'supersecret'\n",
    "\n",
    "# Constantes del dominio\n",
    "WETH         = '0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2'\n",
    "ETH_ADDRESS  = '0x0000000000000000000000000000000000000000'\n",
    "DEAD_ADDRESS = '0x000000000000000000000000000000000000dead'\n",
    "\n",
    "WINDOW     = 6646      # bloques ≈ 24h de observación\n",
    "BLOCKSTUDY = 13220488  # bloque de corte para labeling\n",
    "INACTIVITY = 160000    # bloques sin actividad = pool abandonado\n",
    "\n",
    "# Features del modelo\n",
    "FEATURES = [\n",
    "    'n_syncs', 'WETH', 'prices', 'liquidity',\n",
    "    'num_transactions', 'n_unique_addresses',\n",
    "    'tx_curve', 'mints', 'burns', 'difference_token_pool'\n",
    "]\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print('✓ Configuración OK')\n",
    "print(f'  API     : {API_URL}')\n",
    "print(f'  MLflow  : {MLFLOW_URI}')\n",
    "print(f'  MinIO   : {MINIO_URL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e8ad0",
   "metadata": {},
   "source": [
    "---\n",
    "## Panel 1 — Carga de datos desde la Batch API\n",
    "\n",
    "Cargamos los 12 batches mensuales y los consolidamos en 4 DataFrames globales.\n",
    "Usamos `GET /batch/{id}` para no alterar el puntero del ciclo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d9cb7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API OK — 12 batches disponibles\n",
      "  Rango: 2020-06-24 → 2021-10-16\n"
     ]
    }
   ],
   "source": [
    "def check_api():\n",
    "    \"\"\"Verifica que la API esté disponible y el ciclo inicializado.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(f'{API_URL}/status', timeout=10)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        if not data.get('initialized'):\n",
    "            print('API no inicializada. Ejecutando /init...')\n",
    "            requests.post(f'{API_URL}/init', timeout=300).raise_for_status()\n",
    "            print('✓ Inicializada')\n",
    "        else:\n",
    "            n = data.get('num_batches', 12)\n",
    "            print(f'✓ API OK — {n} batches disponibles')\n",
    "            print(f'  Rango: {data[\"date_min\"]} → {data[\"date_max\"]}')\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f'No se puede conectar a la API ({API_URL}): {e}')\n",
    "\n",
    "check_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "192fa952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos desde la API...\n",
      "  Cargando batch  1/12... 2020-06 | pools=  5 | events=1,127 | transfers=30,044\n",
      "  Cargando batch  2/12... 2020-07 | pools= 21 | events=17,544 | transfers=258,030\n",
      "  Cargando batch  3/12... 2020-08 | pools= 31 | events=31,703 | transfers=415,049\n",
      "  Cargando batch  4/12... 2020-09 | pools= 78 | events=111,803 | transfers=449,695\n",
      "  Cargando batch  5/12... 2020-10 | pools=108 | events=106,462 | transfers=511,996\n",
      "  Cargando batch  6/12... 2020-11 | pools=183 | events=112,299 | transfers=425,357\n",
      "  Cargando batch  7/12... 2020-12 | pools=194 | events=127,761 | transfers=511,303\n",
      "  Cargando batch  8/12... 2021-01 | pools=141 | events=118,165 | transfers=649,453\n",
      "  Cargando batch  9/12... 2021-02 | pools=107 | events=101,843 | transfers=632,556\n",
      "  Cargando batch 10/12... 2021-03 | pools=112 | events=162,492 | transfers=777,381\n",
      "  Cargando batch 11/12... 2021-04 | pools=110 | events=156,935 | transfers=561,107\n",
      "  Cargando batch 12/12... 2021-05 | pools=167 | events=184,207 | transfers=595,073\n",
      "\n",
      "── Totales consolidados ──────────────────\n",
      "Pools     : 617\n",
      "Events    : 1,232,339\n",
      "Metadata  : 998\n",
      "Transfers : 5,817,044\n"
     ]
    }
   ],
   "source": [
    "def load_all_batches(n_batches: int = 12) -> tuple:\n",
    "    \"\"\"\n",
    "    Carga los n_batches mensuales desde la API y los consolida.\n",
    "    Retorna (df_pools, df_events, df_metadata, df_transfers).\n",
    "    \"\"\"\n",
    "    all_pools, all_events, all_metadata, all_transfers = [], [], [], []\n",
    "\n",
    "    for batch_id in range(1, n_batches + 1):\n",
    "        print(f'  Cargando batch {batch_id:2d}/{n_batches}...', end=' ')\n",
    "        r = requests.get(f'{API_URL}/batch/{batch_id}', timeout=600)\n",
    "        r.raise_for_status()\n",
    "        payload = r.json()\n",
    "        month   = payload['month']\n",
    "\n",
    "        data = payload['data']\n",
    "        all_pools.append(pd.DataFrame(data['pools']))\n",
    "        all_events.append(pd.DataFrame(data['events']))\n",
    "        all_metadata.append(pd.DataFrame(data['metadata']))\n",
    "        all_transfers.append(pd.DataFrame(data['transfers']))\n",
    "\n",
    "        s = payload['summary']\n",
    "        print(f'{month} | pools={s[\"n_pools\"]:3d} | events={s[\"n_events\"]:,} | transfers={s[\"n_transfers\"]:,}')\n",
    "\n",
    "    df_pools     = pd.concat(all_pools,     ignore_index=True).drop_duplicates(subset=['pair_address'])\n",
    "    df_events    = pd.concat(all_events,    ignore_index=True).drop_duplicates()\n",
    "    df_metadata  = pd.concat(all_metadata,  ignore_index=True).drop_duplicates(subset=['token_address'])\n",
    "    df_transfers = pd.concat(all_transfers, ignore_index=True).drop_duplicates()\n",
    "\n",
    "    return df_pools, df_events, df_metadata, df_transfers\n",
    "\n",
    "\n",
    "print('Cargando datos desde la API...')\n",
    "df_pools, df_events, df_metadata, df_transfers = load_all_batches()\n",
    "\n",
    "# Columna auxiliar: posición de wETH en el par\n",
    "df_pools['weth_is_token0'] = df_pools['token0'].str.lower() == WETH\n",
    "df_pools['weth_is_token1'] = df_pools['token1'].str.lower() == WETH\n",
    "\n",
    "print(f'\\n── Totales consolidados ──────────────────')\n",
    "print(f'Pools     : {len(df_pools):,}')\n",
    "print(f'Events    : {len(df_events):,}')\n",
    "print(f'Metadata  : {len(df_metadata):,}')\n",
    "print(f'Transfers : {len(df_transfers):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff128ff",
   "metadata": {},
   "source": [
    "---\n",
    "## Panel 2 — Exploración de datos con ydata_profiling\n",
    "\n",
    "Generamos un reporte de exploración para cada uno de los 4 datasets.\n",
    "Los reportes se guardan como HTML para no bloquear el notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3d5efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando reporte: pools (617 filas)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e44cc467e84042a499698fd2457180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 18/18 [00:00<00:00, 603.52it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d44c5017b24ed6a0a067e30ff081ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c4eb2b5cea4937ab14dafc2320e3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae98ecc931ee467bad9f61466884cfeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Guardado: reports/profile_pools.html\n",
      "Generando reporte: events (1,232,339 filas)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4c58713cb74888b74914e79ba52167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [00:15<01:10, 14.19s/it]\u001b[A\n",
      " 33%|███▎      | 2/6 [00:26<00:48, 12.21s/it]\u001b[A\n",
      " 50%|█████     | 3/6 [01:10<01:20, 26.74s/it]\u001b[A\n",
      "100%|██████████| 6/6 [03:43<00:00, 37.28s/it]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fb61cc1b324cdd8fc2a85ab678feeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4a42cf32fe4f7394aaeffd6b230ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5052c1413bb84b76aef3eeed3cdd6232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Guardado: reports/profile_events.html\n",
      "Generando reporte: metadata (998 filas)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769cf4b51c154fe79ecfcf1e4929b4c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:00<00:00, 32.22it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfdce6b102247d28d316a2ce57c75ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93f4c5408b44784bf119b02a1f3ce39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6290fb61032941f5a10f91d628faab6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Guardado: reports/profile_metadata.html\n",
      "Generando reporte: transfers (50,000 filas)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6073b1b81227488ab29c837fcf7153e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 2/8 [00:01<00:03,  1.89it/s]\u001b[A\n",
      " 38%|███▊      | 3/8 [00:01<00:02,  2.20it/s]\u001b[A\n",
      " 50%|█████     | 4/8 [00:01<00:01,  3.10it/s]\u001b[A\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.91it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e4424c4199451795a85f74ab888868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3eeae3979e46ae9cd0389cc004e95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35995a36ca9244b284d1d47089298453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Guardado: reports/profile_transfers.html\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "datasets_to_profile = {\n",
    "    'pools':     df_pools,\n",
    "    'events':    df_events,\n",
    "    'metadata':  df_metadata,\n",
    "    'transfers': df_transfers.sample(min(50_000, len(df_transfers)), random_state=RANDOM_STATE),  # muestra para velocidad\n",
    "}\n",
    "\n",
    "for name, df in datasets_to_profile.items():\n",
    "    print(f'Generando reporte: {name} ({len(df):,} filas)...')\n",
    "    profile = ProfileReport(\n",
    "        df,\n",
    "        title=f'Rug Pull — {name}',\n",
    "        explorative=True,\n",
    "        minimal=(name == 'transfers'),  # transfers: modo minimal por tamaño\n",
    "    )\n",
    "    path = f'reports/profile_{name}.html'\n",
    "    profile.to_file(path)\n",
    "    print(f'  ✓ Guardado: {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f074afe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "── df_pools ──────────────────────────────\n",
      "pair_address        object\n",
      "token0              object\n",
      "token1              object\n",
      "creation_time       object\n",
      "block_number         int64\n",
      "transaction_hash    object\n",
      "token0_decimals      int64\n",
      "token1_decimals      int64\n",
      "sync_count           int64\n",
      "transfer_count       int64\n",
      "burn_count           int64\n",
      "mint_count           int64\n",
      "pair                object\n",
      "estrato             object\n",
      "token_address       object\n",
      "pool_creator        object\n",
      "weth_is_token0        bool\n",
      "weth_is_token1        bool\n",
      "dtype: object\n",
      "                      count unique  \\\n",
      "pair_address            617    617   \n",
      "token0                  617    453   \n",
      "token1                  617    166   \n",
      "creation_time           617    617   \n",
      "block_number     617.000000          \n",
      "transaction_hash        617    617   \n",
      "token0_decimals  617.000000          \n",
      "token1_decimals  617.000000          \n",
      "sync_count       617.000000          \n",
      "transfer_count   617.000000          \n",
      "burn_count       617.000000          \n",
      "mint_count       617.000000          \n",
      "pair                    617    617   \n",
      "estrato                 617      3   \n",
      "token_address           617    617   \n",
      "\n",
      "                                                                top  \\\n",
      "pair_address             0xcb9ca8e060a24708f824a7718c0d678acc89bd13   \n",
      "token0                   0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2   \n",
      "token1                   0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2   \n",
      "creation_time                                   2021-04-12 22:32:06   \n",
      "block_number                                                          \n",
      "transaction_hash  0x9a54c3edbd5c18f4adfe3903e3347712a204aefc90ab...   \n",
      "token0_decimals                                                       \n",
      "token1_decimals                                                       \n",
      "sync_count                                                            \n",
      "transfer_count                                                        \n",
      "burn_count                                                            \n",
      "mint_count                                                            \n",
      "pair              0x2c27bab9b55d3dfe0119b457334c4099e0b4c365_0xc...   \n",
      "estrato                                                        bajo   \n",
      "token_address            0x2c27bab9b55d3dfe0119b457334c4099e0b4c365   \n",
      "\n",
      "                            mean           std  \n",
      "pair_address                                    \n",
      "token0                                          \n",
      "token1                                          \n",
      "creation_time                                   \n",
      "block_number     11211970.035656 532168.249455  \n",
      "transaction_hash                                \n",
      "token0_decimals        17.406807      2.780641  \n",
      "token1_decimals        17.905997      1.188792  \n",
      "sync_count           2581.353323  19681.027961  \n",
      "transfer_count        314.082658   4247.969055  \n",
      "burn_count             47.492707    678.322592  \n",
      "mint_count             83.269044   1056.197086  \n",
      "pair                                            \n",
      "estrato                                         \n",
      "token_address                                   \n"
     ]
    }
   ],
   "source": [
    "# ── Vista rápida de estadísticas básicas\n",
    "print('── df_pools ──────────────────────────────')\n",
    "print(df_pools.dtypes)\n",
    "print(df_pools.describe(include='all').T[['count','unique','top','mean','std']].fillna('').head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d70050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Distribución de eventos por tipo\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "event_counts = df_events['event_type'].value_counts()\n",
    "event_counts.plot(kind='bar', ax=axes[0], color=['#4C72B0','#55A868','#C44E52'])\n",
    "axes[0].set_title('Eventos por tipo')\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "for p in axes[0].patches:\n",
    "    axes[0].annotate(f'{p.get_height():,.0f}', (p.get_x()+0.1, p.get_height()+100))\n",
    "\n",
    "# Eventos por mes\n",
    "df_events['block_number'] = pd.to_numeric(df_events['block_number'], errors='coerce')\n",
    "ETH_REF_BLOCK = 10_000_000\n",
    "ETH_REF_TS    = pd.Timestamp('2020-06-11', tz='UTC')\n",
    "df_events['approx_date'] = ETH_REF_TS + pd.to_timedelta(\n",
    "    (df_events['block_number'] - ETH_REF_BLOCK) * 13.2, unit='s'\n",
    ")\n",
    "events_by_month = df_events.set_index('approx_date').resample('ME')['event_type'].count()\n",
    "events_by_month.plot(kind='bar', ax=axes[1], color='#4C72B0')\n",
    "axes[1].set_title('Eventos por mes')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/eventos_distribucion.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dde9e3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 wallets por volumen enviado:\n",
      "from_address\n",
      "0x0000000000000000000000000000000000000000   1000000015713312231789464112651426712256512.000000\n",
      "0x1964b39279625989ef14697a76b730718b9ee6ed            4353847499896217104062510306164736.000000\n",
      "0xbb2e11998bf694d745b35aa1a00222452024efbf            1759353065452377615452947408224256.000000\n",
      "0x7a250d5630b4cf539739df2c5dacb4c659f2488d            1479765519245565661544290965258240.000000\n",
      "0x798de3403b949bffde1d799d77345642967c5c70            1408341225736259088410639350104064.000000\n",
      "0xa4ed4cfcf8058d48cf5f4bf6c6d36f082f0cf0b8             999999999999999945575230987042816.000000\n",
      "0x0d3004007682f59d52bc57cd677c2537c4637af0             999500000000000034089168850124800.000000\n",
      "0x78de41031c60bde03d47aed04f764669e65fa35e             989694069828253209805875075612672.000000\n",
      "0x61e7e02685bf139df8b6b5b3f927f4d31feb9cef             981916724889527148991967135793152.000000\n",
      "0x7a3ef0de7fd9df8951fafbe8f7ef3240de5837e8             980000000000000027368231689781248.000000\n"
     ]
    }
   ],
   "source": [
    "# ── Análisis de transfers: top wallets por volumen\n",
    "df_transfers['value'] = pd.to_numeric(df_transfers.get('value', df_transfers.get('value_hex', 0)), errors='coerce').fillna(0)\n",
    "\n",
    "top_senders = (\n",
    "    df_transfers.groupby('from_address')['value']\n",
    "    .sum().sort_values(ascending=False).head(10)\n",
    ")\n",
    "print('Top 10 wallets por volumen enviado:')\n",
    "print(top_senders.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac4094",
   "metadata": {},
   "source": [
    "---\n",
    "## Panel 3 — Feature Engineering y Heurísticas de Labeling\n",
    "\n",
    "### Conceptos clave\n",
    "\n",
    "**`WINDOW = 6646 bloques ≈ 24 horas`**  \n",
    "Los patrones de rug pull se empiezan a ver a partir de las ~20 horas de creado el pool. Evaluamos las features exactamente 24h después del primer SYNC del pool.\n",
    "\n",
    "**`tx_curve` (HHI — Herfindahl-Hirschman Index)**  \n",
    "Mide la concentración de tokens en pocas wallets. Un valor cercano a 1 indica que una sola wallet controla casi todo el supply — señal clásica de rug pull.\n",
    "\n",
    "**Tipos de fraude detectados:**\n",
    "- `liquidity_stealing`: el LP retira toda la liquidez de golpe (`liq_mdd = -1.0`, sin recuperación)\n",
    "- `dumping`: el creador vende todos sus tokens hundiendo el precio (`price_mdd ∈ [-1.0, -0.9]`, sin recuperación)\n",
    "\n",
    "**Convención de labels:**\n",
    "- `label = 0` → fraude (rug pull)\n",
    "- `label = 1` → legítimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46bf9781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Funciones de features definidas\n"
     ]
    }
   ],
   "source": [
    "# ── 3.1 Funciones de features\n",
    "\n",
    "def get_pool_features(token_address, df_pools, df_events, eval_block):\n",
    "    \"\"\"Features del pool: reservas, precio y liquidez al momento de evaluación.\"\"\"\n",
    "    pool_info = df_pools[df_pools['token_address'] == token_address].iloc[0]\n",
    "    pair      = pool_info['pair_address']\n",
    "    weth_pos  = 0 if pool_info['weth_is_token0'] else 1\n",
    "    token_pos = 1 - weth_pos\n",
    "    decimals  = pool_info[f'token{token_pos}_decimals']\n",
    "\n",
    "    syncs = df_events[\n",
    "        (df_events['pair_address'] == pair) &\n",
    "        (df_events['event_type']   == 'sync') &\n",
    "        (df_events['block_number'] <  eval_block)\n",
    "    ].sort_values('block_number')\n",
    "\n",
    "    if len(syncs) < 2:\n",
    "        return {}\n",
    "\n",
    "    weth_r  = syncs[f'amount{weth_pos}_or_reserve{weth_pos}_hex'].apply(lambda x: int(x, 16) / 1e18)\n",
    "    token_r = syncs[f'amount{token_pos}_or_reserve{token_pos}_hex'].apply(lambda x: int(x, 16) / 10**decimals)\n",
    "\n",
    "    valid   = (weth_r > 0) & (token_r > 0)\n",
    "    weth_r  = weth_r[valid].values\n",
    "    token_r = token_r[valid].values\n",
    "\n",
    "    if len(weth_r) < 2:\n",
    "        return {}\n",
    "\n",
    "    return {\n",
    "        'n_syncs'  : len(syncs),\n",
    "        'WETH'     : weth_r[-1],\n",
    "        'prices'   : weth_r[-1] / token_r[-1],\n",
    "        'liquidity': weth_r[-1] * token_r[-1],\n",
    "    }\n",
    "\n",
    "\n",
    "def get_transfer_features(token_address, df_transfers, eval_block):\n",
    "    \"\"\"Actividad de transfers: volumen y diversidad de wallets.\"\"\"\n",
    "    t = df_transfers[\n",
    "        (df_transfers['token_address'] == token_address) &\n",
    "        (df_transfers['block_number']  <  eval_block)\n",
    "    ]\n",
    "    n_unique = len(set(t['from_address'].tolist() + t['to_address'].tolist()))\n",
    "    return {\n",
    "        'num_transactions'  : len(t),\n",
    "        'n_unique_addresses': n_unique,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_curve(token_address, df_transfers, eval_block):\n",
    "    \"\"\"\n",
    "    HHI (Herfindahl-Hirschman Index) de concentración del token.\n",
    "    tx_curve ≈ 1.0 → un solo holder controla todo el supply = señal de fraude.\n",
    "    \"\"\"\n",
    "    t = df_transfers[\n",
    "        (df_transfers['token_address'] == token_address) &\n",
    "        (df_transfers['block_number']  <  eval_block)\n",
    "    ].sort_values('block_number')\n",
    "\n",
    "    balances     = defaultdict(float)\n",
    "    total_supply = 0.0\n",
    "\n",
    "    for _, row in t.iterrows():\n",
    "        from_ = row['from_address']\n",
    "        to_   = row['to_address']\n",
    "        value = float(row['value'])\n",
    "        balances[from_] -= value\n",
    "        balances[to_]   += value\n",
    "        if from_ == ETH_ADDRESS:\n",
    "            total_supply += value\n",
    "            balances[from_] = 0\n",
    "        if to_ == ETH_ADDRESS:\n",
    "            total_supply -= value\n",
    "            balances[to_]  = 0\n",
    "\n",
    "    if total_supply == 0:\n",
    "        return {'tx_curve': 1.0}\n",
    "\n",
    "    hhi = sum(\n",
    "        (v / total_supply) ** 2\n",
    "        for addr, v in balances.items()\n",
    "        if addr not in [ETH_ADDRESS, DEAD_ADDRESS]\n",
    "    )\n",
    "    return {'tx_curve': hhi}\n",
    "\n",
    "\n",
    "def get_lp_features(token_address, eval_block, df_pools, df_events, df_metadata):\n",
    "    \"\"\"Features de proveedor de liquidez: mints, burns y diferencia de creación.\"\"\"\n",
    "    pool_info            = df_pools[df_pools['token_address'] == token_address].iloc[0]\n",
    "    pair                 = pool_info['pair_address']\n",
    "    pool_creation_block  = pool_info['block_number']\n",
    "    token_creation_block = df_metadata[\n",
    "        df_metadata['token_address'] == token_address\n",
    "    ]['token_creation_block'].iloc[0]\n",
    "\n",
    "    lp = df_events[\n",
    "        (df_events['pair_address'] == pair) &\n",
    "        (df_events['block_number'] <  eval_block)\n",
    "    ]\n",
    "    return {\n",
    "        'mints'                : len(lp[lp['event_type'] == 'mint']),\n",
    "        'burns'                : len(lp[lp['event_type'] == 'burn']),\n",
    "        'difference_token_pool': pool_creation_block - token_creation_block,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_features(token_address, df_pools, df_events, df_transfers, df_metadata):\n",
    "    \"\"\"Función principal: computa todas las features para un token dado.\"\"\"\n",
    "    pool_info        = df_pools[df_pools['token_address'] == token_address].iloc[0]\n",
    "    pair             = pool_info['pair_address']\n",
    "    first_sync_block = df_events[\n",
    "        (df_events['pair_address'] == pair) &\n",
    "        (df_events['event_type']   == 'sync')\n",
    "    ]['block_number'].min()\n",
    "\n",
    "    eval_block = first_sync_block + WINDOW\n",
    "\n",
    "    features = {}\n",
    "    features.update(get_pool_features(token_address, df_pools, df_events, eval_block))\n",
    "    features.update(get_transfer_features(token_address, df_transfers, eval_block))\n",
    "    features.update(get_curve(token_address, df_transfers, eval_block))\n",
    "    features.update(get_lp_features(token_address, eval_block, df_pools, df_events, df_metadata))\n",
    "    return features\n",
    "\n",
    "\n",
    "print('✓ Funciones de features definidas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4d3aa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Heurísticas de labeling definidas\n"
     ]
    }
   ],
   "source": [
    "# ── 3.2 Heurísticas de labeling\n",
    "\n",
    "def compute_drawdown(series):\n",
    "    \"\"\"Máximo drawdown de una serie de precios o liquidez.\"\"\"\n",
    "    running_max = np.maximum.accumulate(series)\n",
    "    valley_idx  = np.argmax(running_max - series)\n",
    "    peak_idx    = np.argmax(series[:valley_idx]) if valley_idx > 0 else 0\n",
    "    peak_val    = series[peak_idx]\n",
    "    valley_val  = series[valley_idx]\n",
    "    if peak_val == 0:\n",
    "        return 0, peak_idx, valley_idx\n",
    "    return (valley_val - peak_val) / peak_val, peak_idx, valley_idx\n",
    "\n",
    "\n",
    "def compute_recovery(series, peak_idx, valley_idx):\n",
    "    \"\"\"Porcentaje de recuperación después del drawdown.\"\"\"\n",
    "    peak_val   = series[peak_idx]\n",
    "    valley_val = series[valley_idx]\n",
    "    drop = peak_val - valley_val\n",
    "    if drop == 0:\n",
    "        return 0\n",
    "    return (series[-1] - valley_val) / drop\n",
    "\n",
    "\n",
    "def extract_label_features(token_address, df_pools, df_events, blockstudy):\n",
    "    \"\"\"Extrae features de drawdown y recuperación para asignar labels.\"\"\"\n",
    "    pool_info = df_pools[df_pools['token_address'] == token_address].iloc[0]\n",
    "    pair      = pool_info['pair_address']\n",
    "    weth_pos  = 0 if pool_info['weth_is_token0'] else 1\n",
    "    token_pos = 1 - weth_pos\n",
    "    decimals  = pool_info[f'token{token_pos}_decimals']\n",
    "\n",
    "    syncs = df_events[\n",
    "        (df_events['pair_address'] == pair) &\n",
    "        (df_events['event_type']   == 'sync')\n",
    "    ].sort_values('block_number')\n",
    "\n",
    "    if len(syncs) < 5:\n",
    "        return None\n",
    "\n",
    "    weth_r  = syncs[f'amount{weth_pos}_or_reserve{weth_pos}_hex'].apply(lambda x: int(x, 16) / 1e18).values\n",
    "    token_r = syncs[f'amount{token_pos}_or_reserve{token_pos}_hex'].apply(lambda x: int(x, 16) / 10**decimals).values\n",
    "    blocks  = syncs['block_number'].values\n",
    "\n",
    "    valid   = (weth_r > 0) & (token_r > 0)\n",
    "    blocks  = blocks[valid]; weth_r = weth_r[valid]; token_r = token_r[valid]\n",
    "\n",
    "    if len(blocks) < 5:\n",
    "        return None\n",
    "\n",
    "    liquidity = weth_r * token_r\n",
    "    prices    = weth_r / token_r\n",
    "\n",
    "    liq_mdd,   liq_peak,   liq_valley   = compute_drawdown(liquidity)\n",
    "    liq_rc                               = compute_recovery(liquidity, liq_peak, liq_valley)\n",
    "    price_mdd, price_peak, price_valley  = compute_drawdown(prices)\n",
    "    price_rc                             = compute_recovery(prices, price_peak, price_valley)\n",
    "\n",
    "    return {\n",
    "        'token_address': token_address,\n",
    "        'pair_address' : pair,\n",
    "        'inactive'     : int(blockstudy - blocks[-1] > INACTIVITY),\n",
    "        'late_creation': int(blockstudy - blocks[0]  < INACTIVITY),\n",
    "        'liq_mdd'      : liq_mdd,\n",
    "        'liq_rc'       : liq_rc,\n",
    "        'price_mdd'    : price_mdd,\n",
    "        'price_rc'     : price_rc,\n",
    "    }\n",
    "\n",
    "\n",
    "def assign_labels(df_label_features, inactive_transfers):\n",
    "    \"\"\"Asigna label=0 (fraude) según reglas heurísticas.\"\"\"\n",
    "    df = df_label_features.copy()\n",
    "    df['transfer_inactive'] = inactive_transfers\n",
    "    df['fully_inactive']    = (df['inactive'] == 1) & (df['transfer_inactive'] == 1)\n",
    "\n",
    "    eligible = df[(df['fully_inactive']) & (df['late_creation'] == 0)]\n",
    "    records  = []\n",
    "\n",
    "    # Tipo 1: Liquidity Stealing — LP retira toda la liquidez sin recuperación\n",
    "    for token in eligible[\n",
    "        (eligible['liq_mdd'] == -1.0) & (eligible['liq_rc'] <= 0.2)\n",
    "    ].index:\n",
    "        records.append({'token_address': token,\n",
    "                        'pair_address' : df.loc[token, 'pair_address'],\n",
    "                        'label': 0, 'fraud_type': 'liquidity_stealing'})\n",
    "\n",
    "    # Tipo 2: Dumping — creador vende todos sus tokens hundiendo el precio\n",
    "    for token in eligible[\n",
    "        (eligible['liq_mdd'] == 0) &\n",
    "        (eligible['price_mdd'].between(-1.0, -0.9)) &\n",
    "        (eligible['price_rc'].between(0, 0.01))\n",
    "    ].index:\n",
    "        records.append({'token_address': token,\n",
    "                        'pair_address' : df.loc[token, 'pair_address'],\n",
    "                        'label': 0, 'fraud_type': 'dumping'})\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "def build_labels(df_pools, df_events, df_transfers, blockstudy):\n",
    "    \"\"\"Pipeline completo de labeling sobre todos los tokens.\"\"\"\n",
    "    label_features = []\n",
    "    for token in df_pools['token_address']:\n",
    "        result = extract_label_features(token, df_pools, df_events, blockstudy)\n",
    "        if result:\n",
    "            label_features.append(result)\n",
    "\n",
    "    df_lf = pd.DataFrame(label_features).set_index('token_address')\n",
    "\n",
    "    inactive_transfers = (\n",
    "        blockstudy - df_transfers.groupby('token_address')['block_number'].max() > INACTIVITY\n",
    "    ).astype(int)\n",
    "\n",
    "    return assign_labels(df_lf, inactive_transfers)\n",
    "\n",
    "\n",
    "print('✓ Heurísticas de labeling definidas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f44400",
   "metadata": {},
   "source": [
    "---\n",
    "## Panel 4 — Construcción del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ba1fa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construyendo labels...\n",
      "Tokens con fraude detectado: 424\n",
      "fraud_type\n",
      "liquidity_stealing    396\n",
      "dumping                28\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución de labels:\n",
      "label\n",
      "0    424\n",
      "1    193\n",
      "Name: count, dtype: int64\n",
      "  Tasa de fraude: 68.7%\n"
     ]
    }
   ],
   "source": [
    "# ── 4.1 Labeling\n",
    "print('Construyendo labels...')\n",
    "df_labels = build_labels(df_pools, df_events, df_transfers, BLOCKSTUDY)\n",
    "\n",
    "print(f'Tokens con fraude detectado: {len(df_labels)}')\n",
    "print(df_labels['fraud_type'].value_counts())\n",
    "\n",
    "# Todos los tokens parten como legítimos (label=1), luego se sobreescriben los fraudulentos\n",
    "df_all_labels = df_pools[['token_address', 'pair_address']].copy()\n",
    "df_all_labels['label']      = 1\n",
    "df_all_labels['fraud_type'] = 'none'\n",
    "\n",
    "fraud_idx = df_all_labels['token_address'].isin(df_labels['token_address'])\n",
    "df_all_labels.loc[fraud_idx, 'label']      = 0\n",
    "df_all_labels.loc[fraud_idx, 'fraud_type'] = df_all_labels.loc[fraud_idx, 'token_address'].map(\n",
    "    df_labels.set_index('token_address')['fraud_type']\n",
    ")\n",
    "\n",
    "print(f'\\nDistribución de labels:')\n",
    "print(df_all_labels['label'].value_counts())\n",
    "print(f'  Tasa de fraude: {(df_all_labels[\"label\"]==0).mean():.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9679144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando features (puede tardar unos minutos)...\n",
      "  0/617...\n",
      "  100/617...\n",
      "  200/617...\n",
      "  300/617...\n",
      "  400/617...\n",
      "  500/617...\n",
      "  600/617...\n",
      "✓ Features calculadas para 617 tokens\n",
      "                           count                   mean  \\\n",
      "n_syncs               607.000000             166.993410   \n",
      "WETH                  607.000000              57.830233   \n",
      "prices                607.000000 506459213165653.312500   \n",
      "liquidity             607.000000  12981173611022.876953   \n",
      "num_transactions      617.000000             802.047002   \n",
      "n_unique_addresses    617.000000             217.743922   \n",
      "tx_curve              617.000000      4486499684.220627   \n",
      "mints                 617.000000               4.225284   \n",
      "burns                 617.000000               1.252836   \n",
      "difference_token_pool 617.000000          260687.254457   \n",
      "\n",
      "                                          std      min  \\\n",
      "n_syncs                            696.200343 2.000000   \n",
      "WETH                               467.353413 0.000000   \n",
      "prices                7607691298224330.000000 0.000000   \n",
      "liquidity              296233390208212.312500 0.000000   \n",
      "num_transactions                  9264.636762 0.000000   \n",
      "n_unique_addresses                1758.679150 0.000000   \n",
      "tx_curve                  111442216410.192749 0.001123   \n",
      "mints                               22.250262 1.000000   \n",
      "burns                                4.553177 0.000000   \n",
      "difference_token_pool          1034561.200934 0.000000   \n",
      "\n",
      "                                            max  \n",
      "n_syncs                            11226.000000  \n",
      "WETH                               10697.862439  \n",
      "prices                149999999999999968.000000  \n",
      "liquidity               7274325217881250.000000  \n",
      "num_transactions                  226846.000000  \n",
      "n_unique_addresses                 41939.000000  \n",
      "tx_curve                   2768167234076.975098  \n",
      "mints                                384.000000  \n",
      "burns                                 79.000000  \n",
      "difference_token_pool            7557626.000000  \n"
     ]
    }
   ],
   "source": [
    "# ── 4.2 Feature engineering\n",
    "print('Calculando features (puede tardar unos minutos)...')\n",
    "\n",
    "feature_list = []\n",
    "errors = []\n",
    "for i, token in enumerate(df_all_labels['token_address']):\n",
    "    if i % 100 == 0:\n",
    "        print(f'  {i}/{len(df_all_labels)}...')\n",
    "    try:\n",
    "        f = compute_features(token, df_pools, df_events, df_transfers, df_metadata)\n",
    "        if f:\n",
    "            f['token_address'] = token\n",
    "            feature_list.append(f)\n",
    "    except Exception as e:\n",
    "        errors.append({'token': token, 'error': str(e)})\n",
    "\n",
    "df_features = pd.DataFrame(feature_list).set_index('token_address')\n",
    "\n",
    "if errors:\n",
    "    print(f'!Tokens sin features¡: {len(errors)}')\n",
    "\n",
    "print(f'✓ Features calculadas para {len(df_features)} tokens')\n",
    "print(df_features.describe().T[['count','mean','std','min','max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b678ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset final: (607, 12)\n",
      "label\n",
      "0    421\n",
      "1    186\n",
      "Name: count, dtype: int64\n",
      "  label=0 (fraude)   : 421\n",
      "  label=1 (legítimo) : 186\n",
      "  Tasa desbalanceo   : 69.4%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_syncs</th>\n",
       "      <th>WETH</th>\n",
       "      <th>prices</th>\n",
       "      <th>liquidity</th>\n",
       "      <th>num_transactions</th>\n",
       "      <th>n_unique_addresses</th>\n",
       "      <th>tx_curve</th>\n",
       "      <th>mints</th>\n",
       "      <th>burns</th>\n",
       "      <th>difference_token_pool</th>\n",
       "      <th>label</th>\n",
       "      <th>fraud_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_address</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0xdd974d5c2e2928dea5f71b9825b8b646686bd200</th>\n",
       "      <td>56.000000</td>\n",
       "      <td>96.592531</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>3058709.771430</td>\n",
       "      <td>1155</td>\n",
       "      <td>729</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>5827049</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0xa9fbb83a2689f4ff86339a4b96874d718673b627</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.640574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>127601637.622803</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>249969</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2260fac5e5542a773aa44fbcfedf7c193bc2c599</th>\n",
       "      <td>85.000000</td>\n",
       "      <td>394.294949</td>\n",
       "      <td>45.042764</td>\n",
       "      <td>3451.575611</td>\n",
       "      <td>941</td>\n",
       "      <td>234</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>3324813</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1beef31946fbbb40b877a72e4ae04a8d1a5cee06</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.283241</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>8746402.353096</td>\n",
       "      <td>55</td>\n",
       "      <td>40</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5017619</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x08d967bb0134f2d07f7cfb6e246680c53927dd30</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>52.262004</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>3350224.486523</td>\n",
       "      <td>64</td>\n",
       "      <td>31</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1485099</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             n_syncs       WETH    prices  \\\n",
       "token_address                                                               \n",
       "0xdd974d5c2e2928dea5f71b9825b8b646686bd200 56.000000  96.592531  0.003050   \n",
       "0xa9fbb83a2689f4ff86339a4b96874d718673b627 10.000000   2.640574  0.000000   \n",
       "0x2260fac5e5542a773aa44fbcfedf7c193bc2c599 85.000000 394.294949 45.042764   \n",
       "0x1beef31946fbbb40b877a72e4ae04a8d1a5cee06 12.000000   3.283241  0.000001   \n",
       "0x08d967bb0134f2d07f7cfb6e246680c53927dd30 31.000000  52.262004  0.000815   \n",
       "\n",
       "                                                  liquidity  num_transactions  \\\n",
       "token_address                                                                   \n",
       "0xdd974d5c2e2928dea5f71b9825b8b646686bd200   3058709.771430              1155   \n",
       "0xa9fbb83a2689f4ff86339a4b96874d718673b627 127601637.622803                29   \n",
       "0x2260fac5e5542a773aa44fbcfedf7c193bc2c599      3451.575611               941   \n",
       "0x1beef31946fbbb40b877a72e4ae04a8d1a5cee06   8746402.353096                55   \n",
       "0x08d967bb0134f2d07f7cfb6e246680c53927dd30   3350224.486523                64   \n",
       "\n",
       "                                            n_unique_addresses  tx_curve  \\\n",
       "token_address                                                              \n",
       "0xdd974d5c2e2928dea5f71b9825b8b646686bd200                 729  1.000000   \n",
       "0xa9fbb83a2689f4ff86339a4b96874d718673b627                  21  1.000000   \n",
       "0x2260fac5e5542a773aa44fbcfedf7c193bc2c599                 234  1.000000   \n",
       "0x1beef31946fbbb40b877a72e4ae04a8d1a5cee06                  40  1.000000   \n",
       "0x08d967bb0134f2d07f7cfb6e246680c53927dd30                  31  1.000000   \n",
       "\n",
       "                                            mints  burns  \\\n",
       "token_address                                              \n",
       "0xdd974d5c2e2928dea5f71b9825b8b646686bd200     16      0   \n",
       "0xa9fbb83a2689f4ff86339a4b96874d718673b627      3      0   \n",
       "0x2260fac5e5542a773aa44fbcfedf7c193bc2c599     29      1   \n",
       "0x1beef31946fbbb40b877a72e4ae04a8d1a5cee06      5      0   \n",
       "0x08d967bb0134f2d07f7cfb6e246680c53927dd30      3      0   \n",
       "\n",
       "                                            difference_token_pool  label  \\\n",
       "token_address                                                              \n",
       "0xdd974d5c2e2928dea5f71b9825b8b646686bd200                5827049      1   \n",
       "0xa9fbb83a2689f4ff86339a4b96874d718673b627                 249969      1   \n",
       "0x2260fac5e5542a773aa44fbcfedf7c193bc2c599                3324813      1   \n",
       "0x1beef31946fbbb40b877a72e4ae04a8d1a5cee06                5017619      1   \n",
       "0x08d967bb0134f2d07f7cfb6e246680c53927dd30                1485099      1   \n",
       "\n",
       "                                           fraud_type  \n",
       "token_address                                          \n",
       "0xdd974d5c2e2928dea5f71b9825b8b646686bd200       none  \n",
       "0xa9fbb83a2689f4ff86339a4b96874d718673b627       none  \n",
       "0x2260fac5e5542a773aa44fbcfedf7c193bc2c599       none  \n",
       "0x1beef31946fbbb40b877a72e4ae04a8d1a5cee06       none  \n",
       "0x08d967bb0134f2d07f7cfb6e246680c53927dd30       none  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ── 4.3 Dataset final\n",
    "df_dataset = df_features.join(\n",
    "    df_all_labels.set_index('token_address')[['label', 'fraud_type']], how='inner'\n",
    ")\n",
    "\n",
    "# Descartar tokens sin features suficientes a las 24h del pool\n",
    "df_dataset = df_dataset.dropna(subset=['n_syncs', 'WETH', 'prices', 'liquidity'])\n",
    "\n",
    "print(f'Dataset final: {df_dataset.shape}')\n",
    "print(df_dataset['label'].value_counts())\n",
    "print(f'  label=0 (fraude)   : {(df_dataset[\"label\"]==0).sum()}')\n",
    "print(f'  label=1 (legítimo) : {(df_dataset[\"label\"]==1).sum()}')\n",
    "print(f'  Tasa desbalanceo   : {(df_dataset[\"label\"]==0).mean():.1%}')\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c444eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4.4 Visualización del dataset\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feat in enumerate(FEATURES):\n",
    "    for label, color, name in [(0, '#C44E52', 'Fraude'), (1, '#4C72B0', 'Legítimo')]:\n",
    "        data = df_dataset[df_dataset['label'] == label][feat].dropna()\n",
    "        # Escala log si el rango es muy amplio\n",
    "        if data.max() > 1e6:\n",
    "            data = np.log1p(data)\n",
    "            axes[i].set_xlabel(f'log1p({feat})')\n",
    "        axes[i].hist(data, bins=30, alpha=0.6, color=color, label=name, density=True)\n",
    "    axes[i].set_title(feat)\n",
    "    axes[i].legend(fontsize=7)\n",
    "\n",
    "plt.suptitle('Distribución de features por label', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/features_distribucion.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e909d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4.5 Correlaciones\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr = df_dataset[FEATURES + ['label']].corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, vmin=-1, vmax=1, linewidths=0.5)\n",
    "plt.title('Matriz de correlaciones')\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/correlaciones.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf86ca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : (485, 10) | label 0: 336 | label 1: 149\n",
      "Test  : (122, 10)  | label 0: 85  | label 1: 37\n",
      "scale_pos_weight (fraude/legítimo): 2.26\n"
     ]
    }
   ],
   "source": [
    "# ── 4.6 Split train/test\n",
    "X = df_dataset[FEATURES]\n",
    "y = df_dataset['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# Ratio para corregir desbalanceo en modelos basados en árboles\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "print(f'Train : {X_train.shape} | label 0: {(y_train==0).sum()} | label 1: {(y_train==1).sum()}')\n",
    "print(f'Test  : {X_test.shape}  | label 0: {(y_test==0).sum()}  | label 1: {(y_test==1).sum()}')\n",
    "print(f'scale_pos_weight (fraude/legítimo): {scale_pos_weight:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1fba08",
   "metadata": {},
   "source": [
    "---\n",
    "## Panel 5 — Configuración de MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1f06227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Experimento creado: rug-pull-detection\n",
      "✓ MLflow configurado\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "\n",
    "# Crear experimento si no existe\n",
    "EXPERIMENT_NAME = 'rug-pull-detection'\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "if experiment is None:\n",
    "    mlflow.create_experiment(\n",
    "    EXPERIMENT_NAME,\n",
    "    artifact_location=\"s3://mlflows3/artifacts\"\n",
    "    )\n",
    "    print(f'✓ Experimento creado: {EXPERIMENT_NAME}')\n",
    "else:\n",
    "    print(f'✓ Experimento existente: {EXPERIMENT_NAME} (id={experiment.experiment_id})')\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "\n",
    "def log_common_params(window, blockstudy, train_size, test_size, cv_folds, extra_params):\n",
    "    \"\"\"Loggea parámetros comunes a todos los runs.\"\"\"\n",
    "    mlflow.log_param('window_blocks', window)\n",
    "    mlflow.log_param('blockstudy',    blockstudy)\n",
    "    mlflow.log_param('train_size',    train_size)\n",
    "    mlflow.log_param('test_size',     test_size)\n",
    "    mlflow.log_param('cv_folds',      cv_folds)\n",
    "    mlflow.log_param('features',      ','.join(FEATURES))\n",
    "    for k, v in extra_params.items():\n",
    "        mlflow.log_param(k, v)\n",
    "\n",
    "\n",
    "def log_metrics_and_plots(y_test, y_pred, y_proba, cv_aucs, model_name):\n",
    "    \"\"\"Loggea métricas, classification report y confusion matrix.\"\"\"\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    f1  = f1_score(y_test, y_pred, average='macro')\n",
    "    pre = precision_score(y_test, y_pred, average='macro')\n",
    "    rec = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    mlflow.log_metric('cv_auc_mean', cv_aucs.mean())\n",
    "    mlflow.log_metric('cv_auc_std',  cv_aucs.std())\n",
    "    mlflow.log_metric('test_auc',    auc)\n",
    "    mlflow.log_metric('test_f1',     f1)\n",
    "    mlflow.log_metric('test_precision', pre)\n",
    "    mlflow.log_metric('test_recall',    rec)\n",
    "\n",
    "    print(f'CV AUC  : {cv_aucs.mean():.4f} ± {cv_aucs.std():.4f}')\n",
    "    print(f'Test AUC: {auc:.4f}')\n",
    "    print(classification_report(y_test, y_pred, target_names=['Fraude', 'Legítimo']))\n",
    "\n",
    "    # Confusion matrix como artefacto\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_pred,\n",
    "        display_labels=['Fraude', 'Legítimo'],\n",
    "        cmap='Blues', ax=ax\n",
    "    )\n",
    "    ax.set_title(f'{model_name} — Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    path = f'reports/cm_{model_name.lower().replace(\" \",\"_\")}.png'\n",
    "    plt.savefig(path, dpi=100)\n",
    "    mlflow.log_figure(fig, os.path.basename(path))\n",
    "    plt.show()\n",
    "\n",
    "    return auc\n",
    "\n",
    "\n",
    "print('✓ MLflow configurado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7ad82",
   "metadata": {},
   "source": [
    "---\n",
    "## Panel 6 — Modelo 1: Random Forest\n",
    "\n",
    "Baseline robusto, sin hiperparámetros críticos, buena interpretabilidad vía feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432f666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "params_rf = {\n",
    "    'n_estimators'    : 200,\n",
    "    'max_depth'       : 10,\n",
    "    'min_samples_leaf': 2,\n",
    "    'class_weight'    : 'balanced',   # maneja el desbalanceo automáticamente\n",
    "    'random_state'    : RANDOM_STATE,\n",
    "    'n_jobs'          : -1,\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name='random-forest-baseline'):\n",
    "\n",
    "    log_common_params(WINDOW, BLOCKSTUDY, len(X_train), len(X_test), 5, params_rf)\n",
    "    mlflow.log_param('model_type', 'RandomForest')\n",
    "\n",
    "    rf = RandomForestClassifier(**params_rf)\n",
    "\n",
    "    cv_aucs = cross_val_score(rf, X_train, y_train, cv=CV, scoring='roc_auc')\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_rf  = rf.predict(X_test)\n",
    "    y_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    auc_rf = log_metrics_and_plots(y_test, y_pred_rf, y_proba_rf, cv_aucs, 'Random Forest')\n",
    "\n",
    "    # Feature importance\n",
    "    fi = pd.Series(rf.feature_importances_, index=FEATURES).sort_values(ascending=True)\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    fi.plot(kind='barh', ax=ax, color='#4C72B0')\n",
    "    ax.set_title('Random Forest — Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reports/fi_rf.png', dpi=100)\n",
    "    mlflow.log_artifact('reports/fi_rf.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Guardar modelo\n",
    "    mlflow.sklearn.log_model(rf, artifact_path='random_forest')\n",
    "\n",
    "    print(f'\\n✓ Random Forest — Test AUC: {auc_rf:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11bfa56",
   "metadata": {},
   "source": [
    "---\n",
    "## Panel 7 — Modelo 2: XGBoost\n",
    "\n",
    "Gradient boosting. Usa `scale_pos_weight` para corregir el desbalanceo de clases explícitamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgb = {\n",
    "    'n_estimators'    : 100,\n",
    "    'max_depth'       : 6,\n",
    "    'learning_rate'   : 0.1,\n",
    "    'subsample'       : 0.8,\n",
    "    'gamma'           : 1e-2,\n",
    "    'reg_lambda'      : 1.0,\n",
    "    'reg_alpha'       : 1e-2,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_state'    : RANDOM_STATE,\n",
    "    'eval_metric'     : 'auc',\n",
    "    'verbosity'       : 0,\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name='xgboost-baseline'):\n",
    "\n",
    "    log_common_params(WINDOW, BLOCKSTUDY, len(X_train), len(X_test), 5, params_xgb)\n",
    "    mlflow.log_param('model_type', 'XGBoost')\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(**params_xgb)\n",
    "\n",
    "    cv_aucs = cross_val_score(xgb_model, X_train, y_train, cv=CV, scoring='roc_auc')\n",
    "\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_pred_xgb  = xgb_model.predict(X_test)\n",
    "    y_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    auc_xgb = log_metrics_and_plots(y_test, y_pred_xgb, y_proba_xgb, cv_aucs, 'XGBoost')\n",
    "\n",
    "    # Feature importance XGBoost\n",
    "    fi_xgb = pd.Series(xgb_model.feature_importances_, index=FEATURES).sort_values(ascending=True)\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    fi_xgb.plot(kind='barh', ax=ax, color='#55A868')\n",
    "    ax.set_title('XGBoost — Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reports/fi_xgb.png', dpi=100)\n",
    "    mlflow.log_artifact('reports/fi_xgb.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Guardar modelo — usar sklearn flavor para evitar el error de logged-models API\n",
    "    mlflow.sklearn.log_model(xgb_model, artifact_path='xgboost')\n",
    "\n",
    "    print(f'\\n✓ XGBoost — Test AUC: {auc_xgb:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff2fe8",
   "metadata": {},
   "source": [
    "---\n",
    "## Panel 8 — Modelo 3: LightGBM\n",
    "\n",
    "Gradient boosting más rápido que XGBoost en datasets medianos. Usa `is_unbalance=True` para el desbalanceo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb = {\n",
    "    'n_estimators'    : 200,\n",
    "    'max_depth'       : 8,\n",
    "    'learning_rate'   : 0.05,\n",
    "    'num_leaves'      : 31,\n",
    "    'subsample'       : 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha'       : 0.1,\n",
    "    'reg_lambda'      : 1.0,\n",
    "    'is_unbalance'    : True,\n",
    "    'random_state'    : RANDOM_STATE,\n",
    "    'verbose'         : -1,\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name='lightgbm-baseline'):\n",
    "\n",
    "    log_common_params(WINDOW, BLOCKSTUDY, len(X_train), len(X_test), 5, params_lgb)\n",
    "    mlflow.log_param('model_type', 'LightGBM')\n",
    "\n",
    "    lgb_model = lgb.LGBMClassifier(**params_lgb)\n",
    "\n",
    "    cv_aucs = cross_val_score(lgb_model, X_train, y_train, cv=CV, scoring='roc_auc')\n",
    "\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    y_pred_lgb  = lgb_model.predict(X_test)\n",
    "    y_proba_lgb = lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    auc_lgb = log_metrics_and_plots(y_test, y_pred_lgb, y_proba_lgb, cv_aucs, 'LightGBM')\n",
    "\n",
    "    # Feature importance LightGBM\n",
    "    fi_lgb = pd.Series(lgb_model.feature_importances_, index=FEATURES).sort_values(ascending=True)\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    fi_lgb.plot(kind='barh', ax=ax, color='#C44E52')\n",
    "    ax.set_title('LightGBM — Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reports/fi_lgb.png', dpi=100)\n",
    "    mlflow.log_artifact('reports/fi_lgb.png')\n",
    "    plt.show()\n",
    "\n",
    "    mlflow.sklearn.log_model(lgb_model, artifact_path='lightgbm')\n",
    "\n",
    "    print(f'\\n✓ LightGBM — Test AUC: {auc_lgb:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe1f7e1",
   "metadata": {},
   "source": [
    "---\n",
    "## Panel 9 — Comparación de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c6bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Tabla comparativa ─────────────────────────────────────────────────────────\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "results = [\n",
    "    {'Modelo': 'Random Forest', 'AUC': auc_rf,\n",
    "     'F1 (macro)': f1_score(y_test, y_pred_rf, average='macro'),\n",
    "     'Precision (fraude)': precision_score(y_test, y_pred_rf, pos_label=0),\n",
    "     'Recall (fraude)':    recall_score(y_test, y_pred_rf, pos_label=0)},\n",
    "    {'Modelo': 'XGBoost',       'AUC': auc_xgb,\n",
    "     'F1 (macro)': f1_score(y_test, y_pred_xgb, average='macro'),\n",
    "     'Precision (fraude)': precision_score(y_test, y_pred_xgb, pos_label=0),\n",
    "     'Recall (fraude)':    recall_score(y_test, y_pred_xgb, pos_label=0)},\n",
    "    {'Modelo': 'LightGBM',      'AUC': auc_lgb,\n",
    "     'F1 (macro)': f1_score(y_test, y_pred_lgb, average='macro'),\n",
    "     'Precision (fraude)': precision_score(y_test, y_pred_lgb, pos_label=0),\n",
    "     'Recall (fraude)':    recall_score(y_test, y_pred_lgb, pos_label=0)},\n",
    "]\n",
    "\n",
    "df_results = pd.DataFrame(results).set_index('Modelo').round(4)\n",
    "print('── Comparación de modelos ──────────────────────────────')\n",
    "print(df_results.to_string())\n",
    "best_model = df_results['AUC'].idxmax()\n",
    "print(f'\\n Mejor modelo por AUC: {best_model} ({df_results.loc[best_model, \"AUC\"]:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ab49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Curvas ROC superpuestas ───────────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for name, y_proba, color in [\n",
    "    ('Random Forest', y_proba_rf,  '#4C72B0'),\n",
    "    ('XGBoost',       y_proba_xgb, '#55A868'),\n",
    "    ('LightGBM',      y_proba_lgb, '#C44E52'),\n",
    "]:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba, pos_label=0)\n",
    "    auc_val = roc_auc_score(y_test, y_proba)\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC={auc_val:.3f})', color=color, lw=2)\n",
    "\n",
    "ax.plot([0,1],[0,1], 'k--', lw=1, label='Aleatorio')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Curvas ROC — Detección de Rug Pull')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/roc_comparacion.png', dpi=100)\n",
    "plt.show()\n",
    "print('✓ Curvas ROC guardadas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1facb515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Feature importance comparativa ───────────────────────────────────────────\n",
    "df_fi = pd.DataFrame({\n",
    "    'Random Forest': rf.feature_importances_,\n",
    "    'XGBoost'      : xgb_model.feature_importances_,\n",
    "    'LightGBM'     : lgb_model.feature_importances_ / lgb_model.feature_importances_.sum(),\n",
    "}, index=FEATURES)\n",
    "\n",
    "df_fi.sort_values('XGBoost').plot(\n",
    "    kind='barh', figsize=(10, 6), colormap='tab10'\n",
    ")\n",
    "plt.title('Feature Importance comparativa (normalizada)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/fi_comparacion.png', dpi=100)\n",
    "plt.show()\n",
    "\n",
    "print('\\n── Features más importantes (promedio entre modelos) ──')\n",
    "print(df_fi.mean(axis=1).sort_values(ascending=False).round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45555622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Guardar dataset final como artefacto en MLflow ───────────────────────────\n",
    "dataset_path = 'reports/dataset_labeled.csv'\n",
    "df_dataset.reset_index().to_csv(dataset_path, index=False)\n",
    "\n",
    "with mlflow.start_run(run_name='dataset-final'):\n",
    "    mlflow.log_artifact(dataset_path)\n",
    "    mlflow.log_artifact('reports/roc_comparacion.png')\n",
    "    mlflow.log_artifact('reports/fi_comparacion.png')\n",
    "    mlflow.log_artifact('reports/features_distribucion.png')\n",
    "    mlflow.log_artifact('reports/correlaciones.png')\n",
    "    mlflow.log_metric('n_samples',      len(df_dataset))\n",
    "    mlflow.log_metric('n_fraud',        (df_dataset['label']==0).sum())\n",
    "    mlflow.log_metric('n_legit',        (df_dataset['label']==1).sum())\n",
    "    mlflow.log_metric('fraud_rate',     round((df_dataset['label']==0).mean(), 4))\n",
    "    mlflow.log_param('features',        ','.join(FEATURES))\n",
    "    mlflow.log_param('fraud_types',     'liquidity_stealing,dumping')\n",
    "\n",
    "print(f'✓ Dataset y artefactos registrados en MLflow')\n",
    "print(f'  Revisa los resultados en: {MLFLOW_URI}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
